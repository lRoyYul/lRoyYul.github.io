[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Homepage",
    "section": "",
    "text": "Leyi Yu is a PhD candidate in computer engineering at Case Western Reserve University.\n\n\nCurrently Working on\n\n\nDeveloping a modular therapeutic ride-on car platform with 3D-printed adaptive hand controls and immersive driving games to train cognitive and fine motor skills in children with CP or TBI. This system delivers engaging, low-cost, and personalized therapy by combining outdoor driving with indoor game-based training to improve hand function and executive abilities.\nDeveloping and deploying reinforcement learning strategies on humanoid robots\nStudying an electrical-stimulation-based haptic feedback device and completing its development documentation\n\n\nPrevious project\nDeveloped a hapkit-based multi-modal haptic feedback system integrated with a Unity platform jumping game.\n\n\nEducation\nCase Western Reserve University | Cleveland, OH\nPhD in Computer Engineering | In progress"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Download"
  },
  {
    "objectID": "dashboard.html",
    "href": "dashboard.html",
    "title": "Leyi Yu Blog",
    "section": "",
    "text": "library(readr) \nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\nThis chunk loads the TMDB 5000 movie dataset and constructs two key variables for downstream analyses. First, release_decade translates each film’s release_date into a decade factor (1980s through 2020s), which we use as a multi-level grouping variable. Second, english_group classifies films based on original_language, with “English” assigned when the lower-cased code equals “en” and “Non-English” otherwise. These transformations align the raw data with the Study 1 requirements by creating one binary factor for two-group comparisons and one categorical factor for 3–6 group comparisons.\n\ndf &lt;- read_csv(\"data/tmdb_5000_movies.csv\", show_col_types = FALSE)\ndf &lt;- df |&gt;\n  mutate(\n    release_date_parsed = suppressWarnings(ymd(release_date)),\n    release_decade = case_when(\n      year(release_date_parsed) &gt;= 1980 & year(release_date_parsed) &lt; 1990 ~ \"1980s\",\n      year(release_date_parsed) &gt;= 1990 & year(release_date_parsed) &lt; 2000 ~ \"1990s\",\n      year(release_date_parsed) &gt;= 2000 & year(release_date_parsed) &lt; 2010 ~ \"2000s\",\n      year(release_date_parsed) &gt;= 2010 & year(release_date_parsed) &lt; 2020 ~ \"2010s\",\n      year(release_date_parsed) &gt;= 2020 & year(release_date_parsed) &lt; 2030 ~ \"2020s\",\n      TRUE ~ NA_character_\n      ),\n     english_group = ifelse(\n      !is.na(original_language) & tolower(original_language) == \"en\",\n      \"English\", \"Non-English\"\n      )\n  ) \n\ndf_filter &lt;- df |&gt;\n  filter(\n    !is.na(release_date_parsed),\n    !is.na(genres), genres != \"\", genres != \"[]\",\n    !is.na(revenue), revenue &gt;= 0,\n    !is.na(budget),  budget  &gt;= 0,\n    !is.na(vote_average), vote_average &gt;= 0, vote_average &lt;= 10\n  )\n\nThis chunk narrows the data to analytically usable observations. We retain only rows with a parsable release date, nonempty genres, and nonnegative revenue and budget. We also restrict vote_average to the valid [0, 10] range. This filtering step improves internal validity by removing records that would otherwise contribute noise or impossible values, while preserving the sample size needed for the required hypothesis tests and graphics.\n\n\n\nb_ttest &lt;- t.test(\n  vote_average ~ english_group,\n  data = df_filter,\n  conf.level = 0.90\n)\nb_ttest\n\n\n    Welch Two Sample t-test\n\ndata:  vote_average by english_group\nt = -5.3673, df = 329.18, p-value = 1.512e-07\nalternative hypothesis: true difference in means between group English and group Non-English is not equal to 0\n90 percent confidence interval:\n -0.5276455 -0.2795684\nsample estimates:\n    mean in group English mean in group Non-English \n                 6.089010                  6.492617 \n\ndf_filter |&gt;\n  group_by(english_group) |&gt;\n  summarize(\n    n   = n(),\n    mean = mean(vote_average),\n    sd   = sd(vote_average),\n    .groups = \"drop\"\n  )\n\n# A tibble: 2 × 4\n  english_group     n  mean    sd\n  &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 English        4477  6.09  1.13\n2 Non-English     298  6.49  1.27\n\ndf_filter |&gt;\n  ggplot(aes(vote_average, fill = english_group)) +\n  geom_density(alpha = 0.35) +\n  labs(x = \"Vote Average\", y = \"Density\", fill = \"Group\")\n\n\n\n\n\n\n\n\nThis chunk compares mean user ratings between English and Non-English films using a Welch two-sample t-test with a 90% confidence interval. In the filtered data, the English group contains 4,477 films with an average rating of about 6.09 (sd ≈ 1.13), whereas the Non-English group contains 298 films with an average rating of about 6.49 (sd ≈ 1.27). The test statistic is approximately −5.37 with roughly 329 degrees of freedom, yielding a p-value on the order of 10⁻⁷. The 90% confidence interval for the mean difference (English minus Non-English) is roughly [−0.53, −0.28], which excludes zero. Taken together with the density plot—which shows the Non-English distribution shifted to the right—these results indicate that Non-English films receive modestly but credibly higher average ratings than English-language films, and the magnitude of the difference is on the order of four-tenths of a rating point on a 0–10 scale.\n\n\n\n\ndf_c &lt;- df_filter |&gt;\n  mutate(log_revenue = log1p(revenue)) |&gt;\n  filter(!is.na(release_decade)) |&gt;\n  group_by(release_decade) |&gt;\n  mutate(n_decade = n()) |&gt;\n  ungroup() \n\ndf_c |&gt;\n  ggplot(aes(x = release_decade, y = log_revenue)) +\n  geom_boxplot() +\n  labs(x = \"Release decade\", y = \"log(Revenue + 1)\",\n       title = \"Distribution of log(Revenue+1) by decade\")\n\n\n\n\n\n\n\ndf_c |&gt;\n  count(release_decade, sort = TRUE)\n\n# A tibble: 4 × 2\n  release_decade     n\n  &lt;chr&gt;          &lt;int&gt;\n1 2000s           2041\n2 2010s           1429\n3 1990s            776\n4 1980s            277\n\nfit_c &lt;- aov(log_revenue ~ release_decade, data = df_c)\nsummary(fit_c)\n\n                 Df Sum Sq Mean Sq F value Pr(&gt;F)  \nrelease_decade    3    520   173.4   2.585 0.0515 .\nResiduals      4519 303217    67.1                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTukeyHSD(fit_c, conf.level = 0.90)\n\n  Tukey multiple comparisons of means\n    90% family-wise confidence level\n\nFit: aov(formula = log_revenue ~ release_decade, data = df_c)\n\n$release_decade\n                  diff        lwr         upr     p adj\n1990s-1980s -0.7129820 -2.0270391  0.60107511 0.5990527\n2000s-1980s -1.2745683 -2.4767390 -0.07239766 0.0716672\n2010s-1980s -0.8541542 -2.0867035  0.37839513 0.3853463\n2000s-1990s -0.5615863 -1.3533797  0.23020703 0.3641933\n2010s-1990s -0.1411722 -0.9783701  0.69602567 0.9803849\n2010s-2000s  0.4204141 -0.2271726  1.06800082 0.4448412\n\ntbl_c &lt;- df_c |&gt;\n  group_by(release_decade) |&gt;\n  summarize(\n    n    = n(),\n    mean = mean(log_revenue),\n    sd   = sd(log_revenue),\n    .groups = \"drop\"\n  )\ntbl_c\n\n# A tibble: 4 × 4\n  release_decade     n  mean    sd\n  &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1980s            277  13.2  7.57\n2 1990s            776  12.5  8.03\n3 2000s           2041  11.9  8.30\n4 2010s           1429  12.3  8.24\n\n\nThis block examines decade differences in revenue on a log scale. The boxplot of log(Revenue + 1) by release_decade suggests modest shifts across decades, and the one-way ANOVA formalizes that pattern by testing equality of decade means. Tukey’s 90% confidence intervals then identify which specific decade pairs differ; interpret any pair whose interval excludes zero as a credible difference at the assignment’s 90% level, and read the sign of the contrast to determine which decade has the higher mean.\n\n\n\n\ndf_d &lt;- df_filter |&gt;\n  filter(!is.na(english_group)) |&gt;\n  mutate(profit = as.integer(revenue &gt;2.4* budget)) |&gt;\n  select(english_group, profit)\n\ntab_wide &lt;- df_d |&gt;\n  mutate(english_group = factor(english_group, levels = c(\"English\",\"Non-English\"))) |&gt;\n  count(english_group, profit) |&gt;\n  pivot_wider(names_from = profit, values_from = n, values_fill = 0) |&gt;\n  rename(not_profit = `0`, profit = `1`) |&gt;\n  arrange(english_group)\n\ntab_wide \n\n# A tibble: 2 × 3\n  english_group not_profit profit\n  &lt;fct&gt;              &lt;int&gt;  &lt;int&gt;\n1 English             2847   1630\n2 Non-English          222     76\n\na &lt;- tab_wide |&gt; filter(english_group == \"English\")     |&gt; pull(profit)\nb &lt;- tab_wide |&gt; filter(english_group == \"English\")     |&gt; pull(not_profit)\nc &lt;- tab_wide |&gt; filter(english_group == \"Non-English\") |&gt; pull(profit)\nd &lt;- tab_wide |&gt; filter(english_group == \"Non-English\") |&gt; pull(not_profit)\n\n\nc(a=a, b=b, c=c, d=d)\n\n   a    b    c    d \n1630 2847   76  222 \n\np1 &lt;- a/(a+b)  \np2 &lt;- c/(c+d)   \n\nrd_test &lt;- prop.test(x = c(a,c), n = c(a+b, c+d), conf.level = 0.90, correct = TRUE)\nrd_est  &lt;- unname(p1 - p2)\nrd_ci   &lt;- unname(rd_test$conf.int)\n\nRR &lt;- p1/p2\nse_logRR &lt;- sqrt(1/a - 1/(a+b) + 1/c - 1/(c+d))\nz &lt;- qnorm(0.95)  \nRR_ci &lt;- exp(log(RR) + c(-1,1)*z*se_logRR)\n\nor_test &lt;- fisher.test(matrix(c(a,b,c,d), nrow = 2), conf.level = 0.90)\nOR &lt;- unname(or_test$estimate)\nOR_ci &lt;- unname(or_test$conf.int)\n\ntibble::tibble(\n  metric   = c(\"Risk_English (p1)\", \"Risk_NonEnglish (p2)\", \"RD = p1 - p2\", \"RR = p1/p2\", \"OR\"),\n  estimate = c(p1, p2, rd_est, RR, OR),\n  ci90_lwr = c(NA, NA, rd_ci[1], RR_ci[1], OR_ci[1]),\n  ci90_upr = c(NA, NA, rd_ci[2], RR_ci[2], OR_ci[2])\n)\n\n# A tibble: 5 × 4\n  metric               estimate ci90_lwr ci90_upr\n  &lt;chr&gt;                   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Risk_English (p1)       0.364  NA        NA    \n2 Risk_NonEnglish (p2)    0.255  NA        NA    \n3 RD = p1 - p2            0.109   0.0641    0.154\n4 RR = p1/p2              1.43    1.21      1.69 \n5 OR                      1.67    1.33      2.12 \n\ndf_d |&gt;\n  mutate(profit = factor(profit, levels = c(0,1), labels = c(\"Not profit\",\"Profit\"))) |&gt;\n  count(english_group, profit) |&gt;\n  group_by(english_group) |&gt;\n  mutate(pct = n / sum(n)) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = english_group, y = pct, fill = profit)) +\n  geom_col(position = \"fill\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Group\", y = \"Share\", fill = \"\", \n       title = \"Profit vs Not profit proportion by group\")\n\n\n\n\n\n\n\n\nThis block evaluates whether profitability differs by language using a 2×2 table with profit = I(revenue &gt; budget) as the outcome and english_group as the exposure. We report three effect measures with 90% confidence intervals—risk difference (RD), relative risk (RR), and odds ratio (OR). A 90% CI for RD that excludes zero (and RR/OR intervals that exceed one) indicates English-language films have a credibly higher probability of being profitable; if the intervals include the null values, the data do not provide sufficient evidence of a difference at the 90% level.\n\n\n\n\ntop_k &lt;- 5\n\nsafe_parse_genres &lt;- function(s) {\n  if (is.na(s) || s == \"\" || s == \"[]\") return(list())\n  out &lt;- tryCatch(jsonlite::fromJSON(s), error = function(e) NULL)\n  if (is.null(out)) return(list())\n  if (is.data.frame(out) && \"name\" %in% names(out)) {\n    as.character(out$name)\n  } else if (is.list(out)) {\n    unlist(lapply(out, function(x) tryCatch(as.character(x$name), error = function(e) NA_character_)), use.names = FALSE)\n  } else character()\n}\n\ndf_gen_long &lt;- df_filter |&gt;\n  filter(!is.na(release_decade)) |&gt;\n  mutate(genres_vec = purrr::map(genres, safe_parse_genres)) |&gt;\n  tidyr::unnest_longer(genres_vec, values_to = \"genre\") |&gt;\n  filter(!is.na(genre), genre != \"\")\n\ntop_genres &lt;- df_gen_long |&gt;\n  count(genre, sort = TRUE) |&gt;\n  slice_head(n = top_k) |&gt;\n  pull(genre)\n\nheat_rev_mean &lt;- df_gen_long |&gt;\n  filter(genre %in% top_genres) |&gt;\n  mutate(\n    release_decade = factor(release_decade, levels = c(\"1980s\",\"1990s\",\"2000s\",\"2010s\",\"2020s\")),\n    log_revenue = log1p(revenue)\n  ) |&gt;\n  group_by(release_decade, genre) |&gt;\n  summarize(mean_log_rev = mean(log_revenue), .groups = \"drop\")\n\nheat_rev_mean |&gt;\n  ggplot(aes(x = release_decade, y = genre, fill = mean_log_rev)) +\n  geom_tile() +\n  labs(x = \"Decade\", y = \"Genre (TopK)\",\n       fill = \"Mean log(Rev+1)\",\n       title = \"Mean log(Revenue+1) by decade × genre\")\n\n\n\n\n\n\n\nxtab_counts &lt;- df_gen_long |&gt;\n  filter(genre %in% top_genres) |&gt;\n  count(release_decade, genre) |&gt;\n  tidyr::pivot_wider(names_from = genre, values_from = n, values_fill = 0) |&gt;\n  arrange(factor(release_decade, levels = c(\"1980s\",\"1990s\",\"2000s\",\"2010s\",\"2020s\")))\n\nxtab_counts  \n\n# A tibble: 4 × 6\n  release_decade Action Comedy Drama Romance Thriller\n  &lt;chr&gt;           &lt;int&gt;  &lt;int&gt; &lt;int&gt;   &lt;int&gt;    &lt;int&gt;\n1 1980s              84     82   100      37       71\n2 1990s             200    317   396     168      225\n3 2000s             467    809  1015     436      536\n4 2010s             345    461   646     195      396\n\nxtab_rowprop &lt;- xtab_counts |&gt;\n  tibble::column_to_rownames(\"release_decade\") |&gt;\n  as.matrix() |&gt;\n  prop.table(margin = 1) |&gt;\n  as.data.frame() |&gt;\n  tibble::rownames_to_column(\"release_decade\")\n\nxtab_rowprop \n\n  release_decade    Action    Comedy     Drama    Romance  Thriller\n1          1980s 0.2245989 0.2192513 0.2673797 0.09893048 0.1898396\n2          1990s 0.1531394 0.2427259 0.3032159 0.12863706 0.1722818\n3          2000s 0.1431198 0.2479314 0.3110634 0.13361937 0.1642660\n4          2010s 0.1688693 0.2256486 0.3162017 0.09544787 0.1938326\n\nmat_counts &lt;- xtab_counts |&gt;\n  tibble::column_to_rownames(\"release_decade\") |&gt;\n  as.matrix()\n\nchisq_res &lt;- chisq.test(mat_counts) \n\nThis chunk expands multi-label genres into a long format so each film contributes to all of its tags, selects the Top-K genres by overall frequency, and then maps color to the mean of log(Revenue + 1) within each decade–genre cell. Darker tiles indicate decade–genre combinations associated with higher average revenues on the log scale. The follow-up cross-tabulation reports counts by decade and genre together with row-wise proportions, which show how the composition of popular genres shifts over time (for example, Drama typically occupies the largest share and Comedy the second largest, while smaller genres fluctuate in relative weight). A Pearson chi-square test applied to the count table assesses whether decade and genre are independent; the test rejects independence at conventional levels, implying that the distribution of genres changes across decades. Because the heatmap summarizes average log revenue rather than counts, and because each film contributes to all of its genres, these EDA results should be interpreted descriptively: they highlight where revenue tends to be stronger without implying mutually exclusive group membership or causal effects."
  },
  {
    "objectID": "dashboard.html#task-b",
    "href": "dashboard.html#task-b",
    "title": "Leyi Yu Blog",
    "section": "",
    "text": "b_ttest &lt;- t.test(\n  vote_average ~ english_group,\n  data = df_filter,\n  conf.level = 0.90\n)\nb_ttest\n\n\n    Welch Two Sample t-test\n\ndata:  vote_average by english_group\nt = -5.3673, df = 329.18, p-value = 1.512e-07\nalternative hypothesis: true difference in means between group English and group Non-English is not equal to 0\n90 percent confidence interval:\n -0.5276455 -0.2795684\nsample estimates:\n    mean in group English mean in group Non-English \n                 6.089010                  6.492617 \n\ndf_filter |&gt;\n  group_by(english_group) |&gt;\n  summarize(\n    n   = n(),\n    mean = mean(vote_average),\n    sd   = sd(vote_average),\n    .groups = \"drop\"\n  )\n\n# A tibble: 2 × 4\n  english_group     n  mean    sd\n  &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 English        4477  6.09  1.13\n2 Non-English     298  6.49  1.27\n\ndf_filter |&gt;\n  ggplot(aes(vote_average, fill = english_group)) +\n  geom_density(alpha = 0.35) +\n  labs(x = \"Vote Average\", y = \"Density\", fill = \"Group\")\n\n\n\n\n\n\n\n\nThis chunk compares mean user ratings between English and Non-English films using a Welch two-sample t-test with a 90% confidence interval. In the filtered data, the English group contains 4,477 films with an average rating of about 6.09 (sd ≈ 1.13), whereas the Non-English group contains 298 films with an average rating of about 6.49 (sd ≈ 1.27). The test statistic is approximately −5.37 with roughly 329 degrees of freedom, yielding a p-value on the order of 10⁻⁷. The 90% confidence interval for the mean difference (English minus Non-English) is roughly [−0.53, −0.28], which excludes zero. Taken together with the density plot—which shows the Non-English distribution shifted to the right—these results indicate that Non-English films receive modestly but credibly higher average ratings than English-language films, and the magnitude of the difference is on the order of four-tenths of a rating point on a 0–10 scale."
  },
  {
    "objectID": "dashboard.html#task-c",
    "href": "dashboard.html#task-c",
    "title": "Leyi Yu Blog",
    "section": "",
    "text": "df_c &lt;- df_filter |&gt;\n  mutate(log_revenue = log1p(revenue)) |&gt;\n  filter(!is.na(release_decade)) |&gt;\n  group_by(release_decade) |&gt;\n  mutate(n_decade = n()) |&gt;\n  ungroup() \n\ndf_c |&gt;\n  ggplot(aes(x = release_decade, y = log_revenue)) +\n  geom_boxplot() +\n  labs(x = \"Release decade\", y = \"log(Revenue + 1)\",\n       title = \"Distribution of log(Revenue+1) by decade\")\n\n\n\n\n\n\n\ndf_c |&gt;\n  count(release_decade, sort = TRUE)\n\n# A tibble: 4 × 2\n  release_decade     n\n  &lt;chr&gt;          &lt;int&gt;\n1 2000s           2041\n2 2010s           1429\n3 1990s            776\n4 1980s            277\n\nfit_c &lt;- aov(log_revenue ~ release_decade, data = df_c)\nsummary(fit_c)\n\n                 Df Sum Sq Mean Sq F value Pr(&gt;F)  \nrelease_decade    3    520   173.4   2.585 0.0515 .\nResiduals      4519 303217    67.1                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTukeyHSD(fit_c, conf.level = 0.90)\n\n  Tukey multiple comparisons of means\n    90% family-wise confidence level\n\nFit: aov(formula = log_revenue ~ release_decade, data = df_c)\n\n$release_decade\n                  diff        lwr         upr     p adj\n1990s-1980s -0.7129820 -2.0270391  0.60107511 0.5990527\n2000s-1980s -1.2745683 -2.4767390 -0.07239766 0.0716672\n2010s-1980s -0.8541542 -2.0867035  0.37839513 0.3853463\n2000s-1990s -0.5615863 -1.3533797  0.23020703 0.3641933\n2010s-1990s -0.1411722 -0.9783701  0.69602567 0.9803849\n2010s-2000s  0.4204141 -0.2271726  1.06800082 0.4448412\n\ntbl_c &lt;- df_c |&gt;\n  group_by(release_decade) |&gt;\n  summarize(\n    n    = n(),\n    mean = mean(log_revenue),\n    sd   = sd(log_revenue),\n    .groups = \"drop\"\n  )\ntbl_c\n\n# A tibble: 4 × 4\n  release_decade     n  mean    sd\n  &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1980s            277  13.2  7.57\n2 1990s            776  12.5  8.03\n3 2000s           2041  11.9  8.30\n4 2010s           1429  12.3  8.24\n\n\nThis block examines decade differences in revenue on a log scale. The boxplot of log(Revenue + 1) by release_decade suggests modest shifts across decades, and the one-way ANOVA formalizes that pattern by testing equality of decade means. Tukey’s 90% confidence intervals then identify which specific decade pairs differ; interpret any pair whose interval excludes zero as a credible difference at the assignment’s 90% level, and read the sign of the contrast to determine which decade has the higher mean."
  },
  {
    "objectID": "dashboard.html#task-d",
    "href": "dashboard.html#task-d",
    "title": "Leyi Yu Blog",
    "section": "",
    "text": "df_d &lt;- df_filter |&gt;\n  filter(!is.na(english_group)) |&gt;\n  mutate(profit = as.integer(revenue &gt;2.4* budget)) |&gt;\n  select(english_group, profit)\n\ntab_wide &lt;- df_d |&gt;\n  mutate(english_group = factor(english_group, levels = c(\"English\",\"Non-English\"))) |&gt;\n  count(english_group, profit) |&gt;\n  pivot_wider(names_from = profit, values_from = n, values_fill = 0) |&gt;\n  rename(not_profit = `0`, profit = `1`) |&gt;\n  arrange(english_group)\n\ntab_wide \n\n# A tibble: 2 × 3\n  english_group not_profit profit\n  &lt;fct&gt;              &lt;int&gt;  &lt;int&gt;\n1 English             2847   1630\n2 Non-English          222     76\n\na &lt;- tab_wide |&gt; filter(english_group == \"English\")     |&gt; pull(profit)\nb &lt;- tab_wide |&gt; filter(english_group == \"English\")     |&gt; pull(not_profit)\nc &lt;- tab_wide |&gt; filter(english_group == \"Non-English\") |&gt; pull(profit)\nd &lt;- tab_wide |&gt; filter(english_group == \"Non-English\") |&gt; pull(not_profit)\n\n\nc(a=a, b=b, c=c, d=d)\n\n   a    b    c    d \n1630 2847   76  222 \n\np1 &lt;- a/(a+b)  \np2 &lt;- c/(c+d)   \n\nrd_test &lt;- prop.test(x = c(a,c), n = c(a+b, c+d), conf.level = 0.90, correct = TRUE)\nrd_est  &lt;- unname(p1 - p2)\nrd_ci   &lt;- unname(rd_test$conf.int)\n\nRR &lt;- p1/p2\nse_logRR &lt;- sqrt(1/a - 1/(a+b) + 1/c - 1/(c+d))\nz &lt;- qnorm(0.95)  \nRR_ci &lt;- exp(log(RR) + c(-1,1)*z*se_logRR)\n\nor_test &lt;- fisher.test(matrix(c(a,b,c,d), nrow = 2), conf.level = 0.90)\nOR &lt;- unname(or_test$estimate)\nOR_ci &lt;- unname(or_test$conf.int)\n\ntibble::tibble(\n  metric   = c(\"Risk_English (p1)\", \"Risk_NonEnglish (p2)\", \"RD = p1 - p2\", \"RR = p1/p2\", \"OR\"),\n  estimate = c(p1, p2, rd_est, RR, OR),\n  ci90_lwr = c(NA, NA, rd_ci[1], RR_ci[1], OR_ci[1]),\n  ci90_upr = c(NA, NA, rd_ci[2], RR_ci[2], OR_ci[2])\n)\n\n# A tibble: 5 × 4\n  metric               estimate ci90_lwr ci90_upr\n  &lt;chr&gt;                   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Risk_English (p1)       0.364  NA        NA    \n2 Risk_NonEnglish (p2)    0.255  NA        NA    \n3 RD = p1 - p2            0.109   0.0641    0.154\n4 RR = p1/p2              1.43    1.21      1.69 \n5 OR                      1.67    1.33      2.12 \n\ndf_d |&gt;\n  mutate(profit = factor(profit, levels = c(0,1), labels = c(\"Not profit\",\"Profit\"))) |&gt;\n  count(english_group, profit) |&gt;\n  group_by(english_group) |&gt;\n  mutate(pct = n / sum(n)) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = english_group, y = pct, fill = profit)) +\n  geom_col(position = \"fill\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Group\", y = \"Share\", fill = \"\", \n       title = \"Profit vs Not profit proportion by group\")\n\n\n\n\n\n\n\n\nThis block evaluates whether profitability differs by language using a 2×2 table with profit = I(revenue &gt; budget) as the outcome and english_group as the exposure. We report three effect measures with 90% confidence intervals—risk difference (RD), relative risk (RR), and odds ratio (OR). A 90% CI for RD that excludes zero (and RR/OR intervals that exceed one) indicates English-language films have a credibly higher probability of being profitable; if the intervals include the null values, the data do not provide sufficient evidence of a difference at the 90% level."
  },
  {
    "objectID": "dashboard.html#task-e",
    "href": "dashboard.html#task-e",
    "title": "Leyi Yu Blog",
    "section": "",
    "text": "top_k &lt;- 5\n\nsafe_parse_genres &lt;- function(s) {\n  if (is.na(s) || s == \"\" || s == \"[]\") return(list())\n  out &lt;- tryCatch(jsonlite::fromJSON(s), error = function(e) NULL)\n  if (is.null(out)) return(list())\n  if (is.data.frame(out) && \"name\" %in% names(out)) {\n    as.character(out$name)\n  } else if (is.list(out)) {\n    unlist(lapply(out, function(x) tryCatch(as.character(x$name), error = function(e) NA_character_)), use.names = FALSE)\n  } else character()\n}\n\ndf_gen_long &lt;- df_filter |&gt;\n  filter(!is.na(release_decade)) |&gt;\n  mutate(genres_vec = purrr::map(genres, safe_parse_genres)) |&gt;\n  tidyr::unnest_longer(genres_vec, values_to = \"genre\") |&gt;\n  filter(!is.na(genre), genre != \"\")\n\ntop_genres &lt;- df_gen_long |&gt;\n  count(genre, sort = TRUE) |&gt;\n  slice_head(n = top_k) |&gt;\n  pull(genre)\n\nheat_rev_mean &lt;- df_gen_long |&gt;\n  filter(genre %in% top_genres) |&gt;\n  mutate(\n    release_decade = factor(release_decade, levels = c(\"1980s\",\"1990s\",\"2000s\",\"2010s\",\"2020s\")),\n    log_revenue = log1p(revenue)\n  ) |&gt;\n  group_by(release_decade, genre) |&gt;\n  summarize(mean_log_rev = mean(log_revenue), .groups = \"drop\")\n\nheat_rev_mean |&gt;\n  ggplot(aes(x = release_decade, y = genre, fill = mean_log_rev)) +\n  geom_tile() +\n  labs(x = \"Decade\", y = \"Genre (TopK)\",\n       fill = \"Mean log(Rev+1)\",\n       title = \"Mean log(Revenue+1) by decade × genre\")\n\n\n\n\n\n\n\nxtab_counts &lt;- df_gen_long |&gt;\n  filter(genre %in% top_genres) |&gt;\n  count(release_decade, genre) |&gt;\n  tidyr::pivot_wider(names_from = genre, values_from = n, values_fill = 0) |&gt;\n  arrange(factor(release_decade, levels = c(\"1980s\",\"1990s\",\"2000s\",\"2010s\",\"2020s\")))\n\nxtab_counts  \n\n# A tibble: 4 × 6\n  release_decade Action Comedy Drama Romance Thriller\n  &lt;chr&gt;           &lt;int&gt;  &lt;int&gt; &lt;int&gt;   &lt;int&gt;    &lt;int&gt;\n1 1980s              84     82   100      37       71\n2 1990s             200    317   396     168      225\n3 2000s             467    809  1015     436      536\n4 2010s             345    461   646     195      396\n\nxtab_rowprop &lt;- xtab_counts |&gt;\n  tibble::column_to_rownames(\"release_decade\") |&gt;\n  as.matrix() |&gt;\n  prop.table(margin = 1) |&gt;\n  as.data.frame() |&gt;\n  tibble::rownames_to_column(\"release_decade\")\n\nxtab_rowprop \n\n  release_decade    Action    Comedy     Drama    Romance  Thriller\n1          1980s 0.2245989 0.2192513 0.2673797 0.09893048 0.1898396\n2          1990s 0.1531394 0.2427259 0.3032159 0.12863706 0.1722818\n3          2000s 0.1431198 0.2479314 0.3110634 0.13361937 0.1642660\n4          2010s 0.1688693 0.2256486 0.3162017 0.09544787 0.1938326\n\nmat_counts &lt;- xtab_counts |&gt;\n  tibble::column_to_rownames(\"release_decade\") |&gt;\n  as.matrix()\n\nchisq_res &lt;- chisq.test(mat_counts) \n\nThis chunk expands multi-label genres into a long format so each film contributes to all of its tags, selects the Top-K genres by overall frequency, and then maps color to the mean of log(Revenue + 1) within each decade–genre cell. Darker tiles indicate decade–genre combinations associated with higher average revenues on the log scale. The follow-up cross-tabulation reports counts by decade and genre together with row-wise proportions, which show how the composition of popular genres shifts over time (for example, Drama typically occupies the largest share and Comedy the second largest, while smaller genres fluctuate in relative weight). A Pearson chi-square test applied to the count table assesses whether decade and genre are independent; the test rejects independence at conventional levels, implying that the distribution of genres changes across decades. Because the heatmap summarizes average log revenue rather than counts, and because each film contributes to all of its genres, these EDA results should be interpreted descriptively: they highlight where revenue tends to be stronger without implying mutually exclusive group membership or causal effects."
  },
  {
    "objectID": "study2.html",
    "href": "study2.html",
    "title": "study2",
    "section": "",
    "text": "library(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(mice)\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\nlibrary(broom)\nlibrary(ggplot2)\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\ndf &lt;- read_csv(\"data/tmdb_5000_movies.csv\", show_col_types = FALSE)\ndf &lt;- df |&gt;\n  mutate(\n    release_date_parsed = suppressWarnings(ymd(release_date)),\n    release_year = year(release_date_parsed)\n  )\n  \ndf_filter &lt;- df |&gt;\n  filter(\n    !is.na(release_date_parsed),\n    !is.na(release_year),\n    release_year &gt;= 1980,\n    release_year &lt; 2020,                 # ← 超出范围的行直接丢弃\n    !is.na(genres), genres != \"\", genres != \"[]\",\n    !is.na(revenue), revenue &gt;= 0,\n    !is.na(budget),  budget  &gt;= 0,\n    !is.na(vote_average), vote_average &gt;= 0, vote_average &lt;= 10\n  ) |&gt;\n  mutate(\n    release_decade = case_when(\n      release_year &gt;= 1980 & release_year &lt; 1990 ~ \"1980s\",\n      release_year &gt;= 1990 & release_year &lt; 2000 ~ \"1990s\",\n      release_year &gt;= 2000 & release_year &lt; 2010 ~ \"2000s\",\n      release_year &gt;= 2010 & release_year &lt; 2020 ~ \"2010s\",\n      TRUE ~ NA_character_  # 现在基本不会命中\n    ),\n    english_group = ifelse(\n      !is.na(original_language) & tolower(original_language) == \"en\",\n      \"English\", \"Non-English\"\n    )\n  )\n\n\ns2_base &lt;- df_filter |&gt;\n  mutate(\n    log_revenue = log1p(revenue),\n    log_budget  = log1p(budget),\n    log_votes   = log1p(vote_count)\n  ) |&gt;\n  select(\n    id,                      # subject ID\n    log_revenue,             # Outcome（连续）\n    vote_average,            # Key predictor（连续）\n    log_budget, runtime, log_votes, popularity,   # 其它预测（连续）\n    release_decade, english_group                  # 其它预测（分类型）\n  ) |&gt;\n  filter(is.finite(log_revenue), !is.na(vote_average))\n\nnrow(s2_base); s2_base |&gt; names()\n\n[1] 4523\n\n\n[1] \"id\"             \"log_revenue\"    \"vote_average\"   \"log_budget\"    \n[5] \"runtime\"        \"log_votes\"      \"popularity\"     \"release_decade\"\n[9] \"english_group\" \n\n\n\ns2_key  &lt;- s2_base |&gt; select(id, log_revenue, vote_average)\ns2_ctrl &lt;- s2_base |&gt; select(-id, -log_revenue, -vote_average)\n\nmiss_before &lt;- sapply(s2_ctrl, \\(x) sum(is.na(x)))\n\nimp &lt;- mice(s2_ctrl, m = 1, method = \"pmm\", maxit = 5, printFlag = FALSE)\n\nWarning: Number of logged events: 2\n\ns2_ctrl_imp &lt;- complete(imp, 1)\n\ns2_imp &lt;- dplyr::bind_cols(s2_key, s2_ctrl_imp)\n\nmiss_after &lt;- sapply(s2_ctrl_imp, \\(x) sum(is.na(x)))\nimpute_report &lt;- tibble::tibble(\n  variable = names(miss_before),\n  missing_before = as.integer(miss_before),\n  missing_after  = as.integer(miss_after),\n  imputed_n      = pmax(missing_before - missing_after, 0L)\n)\nimpute_report\n\n# A tibble: 6 × 4\n  variable       missing_before missing_after imputed_n\n  &lt;chr&gt;                   &lt;int&gt;         &lt;int&gt;     &lt;int&gt;\n1 log_budget                  0             0         0\n2 runtime                     2             0         2\n3 log_votes                   0             0         0\n4 popularity                  0             0         0\n5 release_decade              0             0         0\n6 english_group               0             0         0\n\nnrow(s2_imp)\n\n[1] 4523\n\n\n\nset.seed(2025)\ntraining_sample &lt;- s2_imp |&gt; slice_sample(prop = 0.75)\ntest_sample     &lt;- anti_join(s2_imp, training_sample, by = \"id\")\n\n# 整理因子水平\ntraining_sample &lt;- training_sample |&gt;\n  mutate(\n    release_decade = factor(release_decade, levels = c(\"1980s\",\"1990s\",\"2000s\",\"2010s\",\"2020s\")),\n    english_group  = factor(english_group,  levels = c(\"English\",\"Non-English\"))\n  )\ntest_sample &lt;- test_sample |&gt;\n  mutate(\n    release_decade = factor(release_decade, levels = levels(training_sample$release_decade)),\n    english_group  = factor(english_group,  levels = levels(training_sample$english_group))\n  )\n\nnrow(training_sample); nrow(test_sample)\n\n[1] 3392\n\n\n[1] 1131\n\n\n\n# 原始 revenue 与 log(Revenue+1) 的分布对比\nlibrary(broom)\n\nfit_full &lt;- lm(\n  log_revenue ~ vote_average + log_budget + runtime + log_votes +\n    popularity + release_decade + english_group,\n  data = training_sample\n)\n\nfit_subset &lt;- lm(\n  log_revenue ~ vote_average,\n  data = training_sample\n)\n\nfull_ci90   &lt;- tidy(fit_full,   conf.int = TRUE, conf.level = 0.90)\nsubset_ci90 &lt;- tidy(fit_subset, conf.int = TRUE, conf.level = 0.90)\n\nfull_ci90   |&gt; filter(term == \"vote_average\")\n\n# A tibble: 1 × 7\n  term         estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 vote_average   -0.214    0.0951     -2.25  0.0245   -0.370   -0.0574\n\nsubset_ci90 |&gt; filter(term == \"vote_average\")\n\n# A tibble: 1 × 7\n  term         estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 vote_average     1.93     0.121      15.9 3.37e-55     1.73      2.12\n\n\n\n# log 尺度\npred_full_log   &lt;- predict(fit_full,   newdata = test_sample)\npred_subset_log &lt;- predict(fit_subset, newdata = test_sample)\n\nrmse_log &lt;- function(y, yhat) sqrt(mean((y - yhat)^2))\nmae_log  &lt;- function(y, yhat) mean(abs(y - yhat))\nr2_log   &lt;- function(y, yhat) cor(y, yhat)^2\n\nlog_metrics &lt;- tibble::tibble(\n  model = c(\"Full\",\"Subset\"),\n  RMSE_log = c(rmse_log(test_sample$log_revenue, pred_full_log),\n               rmse_log(test_sample$log_revenue, pred_subset_log)),\n  MAE_log  = c(mae_log(test_sample$log_revenue, pred_full_log),\n               mae_log(test_sample$log_revenue, pred_subset_log)),\n  R2_log   = c(r2_log(test_sample$log_revenue, pred_full_log),\n               r2_log(test_sample$log_revenue, pred_subset_log))\n)\n\n# 原尺度（Duan smearing）\nsmear_full   &lt;- mean(exp(residuals(fit_full)))\nsmear_subset &lt;- mean(exp(residuals(fit_subset)))\n\npred_full_raw   &lt;- exp(pred_full_log)   * smear_full   - 1\npred_subset_raw &lt;- exp(pred_subset_log) * smear_subset - 1\ntruth_raw       &lt;- exp(test_sample$log_revenue) - 1\n\nrmse &lt;- function(y, yhat) sqrt(mean((y - yhat)^2))\nmae  &lt;- function(y, yhat) mean(abs(y - yhat))\nr2   &lt;- function(y, yhat) cor(y, yhat)^2\n\nraw_metrics &lt;- tibble::tibble(\n  model = c(\"Full\",\"Subset\"),\n  RMSE  = c(rmse(truth_raw, pred_full_raw),\n            rmse(truth_raw, pred_subset_raw)),\n  MAE   = c(mae(truth_raw,  pred_full_raw),\n            mae(truth_raw,  pred_subset_raw)),\n  R2    = c(r2(truth_raw,   pred_full_raw),\n            r2(truth_raw,   pred_subset_raw))\n)\n\nlog_metrics\n\n# A tibble: 2 × 4\n  model  RMSE_log MAE_log R2_log\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 Full       5.00    3.51  0.630\n2 Subset     7.79    6.83  0.102\n\nraw_metrics\n\n# A tibble: 2 × 4\n  model     RMSE     MAE     R2\n  &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 Full   3.29e13 6.00e12 0.454 \n2 Subset 1.08e 9 4.65e 8 0.0314"
  },
  {
    "objectID": "dash1.html",
    "href": "dash1.html",
    "title": "lRoyYul",
    "section": "",
    "text": "Welch t testgroup summarydensity plot\n\n\n\n\n\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  vote_average by english_group\nt = -5.3673, df = 329.18, p-value = 1.512e-07\nalternative hypothesis: true difference in means between group English and group Non-English is not equal to 0\n90 percent confidence interval:\n -0.5276455 -0.2795684\nsample estimates:\n    mean in group English mean in group Non-English \n                 6.089010                  6.492617 \n\n\n\n    \n    \n        \n    \n\n\n\n\n\n\n\n# A tibble: 2 × 4\n  english_group     n  mean    sd\n  &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 English        4477  6.09  1.13\n2 Non-English     298  6.49  1.27\n\n\n\n    \n    \n        \n    \n\n\n\n\n\n\n\n\n\n\n\n    \n    \n        \n    \n\n\n\n    \n    \n        \n    \n\n\n\n\n\nThis chunk compares mean user ratings between English and Non-English films using a Welch two-sample t-test with a 90% confidence interval. In the filtered data, the English group contains 4,477 films with an average rating of about 6.09 (sd ≈ 1.13), whereas the Non-English group contains 298 films with an average rating of about 6.49 (sd ≈ 1.27). The test statistic is approximately −5.37 with roughly 329 degrees of freedom, yielding a p-value on the order of 10⁻⁷. The 90% confidence interval for the mean difference (English minus Non-English) is roughly [−0.53, −0.28], which excludes zero. Taken together with the density plot—which shows the Non-English distribution shifted to the right—these results indicate that Non-English films receive modestly but credibly higher average ratings than English-language films, and the magnitude of the difference is on the order of four-tenths of a rating point on a 0–10 scale.\n\n\n    \n    \n        \n    \n\n\n\n\n\n\n\n\n\n\n\ngraphANOVA& Tukey\n\n\n\n\n\n\n\n\n\n\n# A tibble: 4 × 2\n  release_decade     n\n  &lt;chr&gt;          &lt;int&gt;\n1 2000s           2041\n2 2010s           1429\n3 1990s            776\n4 1980s            277\n\n\n\n    \n    \n        \n    \n\n\n\n\n\n\n\n                 Df Sum Sq Mean Sq F value Pr(&gt;F)  \nrelease_decade    3    520   173.4   2.585 0.0515 .\nResiduals      4519 303217    67.1                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n  Tukey multiple comparisons of means\n    90% family-wise confidence level\n\nFit: aov(formula = log_revenue ~ release_decade, data = df_c)\n\n$release_decade\n                  diff        lwr         upr     p adj\n1990s-1980s -0.7129820 -2.0270391  0.60107511 0.5990527\n2000s-1980s -1.2745683 -2.4767390 -0.07239766 0.0716672\n2010s-1980s -0.8541542 -2.0867035  0.37839513 0.3853463\n2000s-1990s -0.5615863 -1.3533797  0.23020703 0.3641933\n2010s-1990s -0.1411722 -0.9783701  0.69602567 0.9803849\n2010s-2000s  0.4204141 -0.2271726  1.06800082 0.4448412\n\n\n# A tibble: 4 × 4\n  release_decade     n  mean    sd\n  &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1980s            277  13.2  7.57\n2 1990s            776  12.5  8.03\n3 2000s           2041  11.9  8.30\n4 2010s           1429  12.3  8.24\n\n\n\n    \n    \n        \n    \n\n\n\n    \n    \n        \n    \n\n\nThis block examines decade differences in revenue on a log scale. The boxplot of log(Revenue + 1) by release_decade suggests modest shifts across decades, and the one-way ANOVA formalizes that pattern by testing equality of decade means. Tukey’s 90% confidence intervals then identify which specific decade pairs differ; interpret any pair whose interval excludes zero as a credible difference at the assignment’s 90% level, and read the sign of the contrast to determine which decade has the higher mean.\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 2 × 3\n  english_group not_profit profit\n  &lt;fct&gt;              &lt;int&gt;  &lt;int&gt;\n1 English             2847   1630\n2 Non-English          222     76\n\n\n   a    b    c    d \n1630 2847   76  222 \n\n\n# A tibble: 5 × 4\n  metric               estimate ci90_lwr ci90_upr\n  &lt;chr&gt;                   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Risk_English (p1)       0.364  NA        NA    \n2 Risk_NonEnglish (p2)    0.255  NA        NA    \n3 RD = p1 - p2            0.109   0.0641    0.154\n4 RR = p1/p2              1.43    1.21      1.69 \n5 OR                      1.67    1.33      2.12 \n\n\n\n\n\n\n    \n    \n        \n    \n\n\nThis block evaluates whether profitability differs by language using a 2×2 table with profit = I(revenue &gt; budget) as the outcome and english_group as the exposure. We report three effect measures with 90% confidence intervals—risk difference (RD), relative risk (RR), and odds ratio (OR). A 90% CI for RD that excludes zero (and RR/OR intervals that exceed one) indicates English-language films have a credibly higher probability of being profitable; if the intervals include the null values, the data do not provide sufficient evidence of a difference at the 90% level.\n\n\n    \n    \n        \n    \n\n\n\n\n\n\n\n\n\n# A tibble: 4 × 6\n  release_decade Action Comedy Drama Romance Thriller\n  &lt;chr&gt;           &lt;int&gt;  &lt;int&gt; &lt;int&gt;   &lt;int&gt;    &lt;int&gt;\n1 1980s              84     82   100      37       71\n2 1990s             200    317   396     168      225\n3 2000s             467    809  1015     436      536\n4 2010s             345    461   646     195      396\n\n\n  release_decade    Action    Comedy     Drama    Romance  Thriller\n1          1980s 0.2245989 0.2192513 0.2673797 0.09893048 0.1898396\n2          1990s 0.1531394 0.2427259 0.3032159 0.12863706 0.1722818\n3          2000s 0.1431198 0.2479314 0.3110634 0.13361937 0.1642660\n4          2010s 0.1688693 0.2256486 0.3162017 0.09544787 0.1938326\n\n\n\n    \n    \n        \n    \n\n\nThis chunk expands multi-label genres into a long format so each film contributes to all of its tags, selects the Top-K genres by overall frequency, and then maps color to the mean of log(Revenue + 1) within each decade–genre cell. Darker tiles indicate decade–genre combinations associated with higher average revenues on the log scale. The follow-up cross-tabulation reports counts by decade and genre together with row-wise proportions, which show how the composition of popular genres shifts over time (for example, Drama typically occupies the largest share and Comedy the second largest, while smaller genres fluctuate in relative weight). A Pearson chi-square test applied to the count table assesses whether decade and genre are independent; the test rejects independence at conventional levels, implying that the distribution of genres changes across decades. Because the heatmap summarizes average log revenue rather than counts, and because each film contributes to all of its genres, these EDA results should be interpreted descriptively: they highlight where revenue tends to be stronger without implying mutually exclusive group membership or causal effects."
  },
  {
    "objectID": "dash1.html#task-b",
    "href": "dash1.html#task-b",
    "title": "dash1",
    "section": "",
    "text": "b_ttest &lt;- t.test(\n  vote_average ~ english_group,\n  data = df_filter,\n  conf.level = 0.90\n)\nb_ttest\n\n\n    Welch Two Sample t-test\n\ndata:  vote_average by english_group\nt = -5.3673, df = 329.18, p-value = 1.512e-07\nalternative hypothesis: true difference in means between group English and group Non-English is not equal to 0\n90 percent confidence interval:\n -0.5276455 -0.2795684\nsample estimates:\n    mean in group English mean in group Non-English \n                 6.089010                  6.492617 \n\n\n\n\n\n\ndf_filter |&gt;\n  group_by(english_group) |&gt;\n  summarize(\n    n   = n(),\n    mean = mean(vote_average),\n    sd   = sd(vote_average),\n    .groups = \"drop\"\n  )\n\n# A tibble: 2 × 4\n  english_group     n  mean    sd\n  &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 English        4477  6.09  1.13\n2 Non-English     298  6.49  1.27\n\n\n\n\n\n\ndf_filter |&gt;\n  ggplot(aes(vote_average, fill = english_group)) +\n  geom_density(alpha = 0.35) +\n  labs(x = \"Vote Average\", y = \"Density\", fill = \"Group\")\n\n\n\n\n\n\n\n\nThis chunk compares mean user ratings between English and Non-English films using a Welch two-sample t-test with a 90% confidence interval. In the filtered data, the English group contains 4,477 films with an average rating of about 6.09 (sd ≈ 1.13), whereas the Non-English group contains 298 films with an average rating of about 6.49 (sd ≈ 1.27). The test statistic is approximately −5.37 with roughly 329 degrees of freedom, yielding a p-value on the order of 10⁻⁷. The 90% confidence interval for the mean difference (English minus Non-English) is roughly [−0.53, −0.28], which excludes zero. Taken together with the density plot—which shows the Non-English distribution shifted to the right—these results indicate that Non-English films receive modestly but credibly higher average ratings than English-language films, and the magnitude of the difference is on the order of four-tenths of a rating point on a 0–10 scale."
  },
  {
    "objectID": "dash1.html#task-c",
    "href": "dash1.html#task-c",
    "title": "dash1",
    "section": "",
    "text": "df_c &lt;- df_filter |&gt;\n  mutate(log_revenue = log1p(revenue)) |&gt;\n  filter(!is.na(release_decade)) |&gt;\n  group_by(release_decade) |&gt;\n  mutate(n_decade = n()) |&gt;\n  ungroup() \n\ndf_c |&gt;\n  ggplot(aes(x = release_decade, y = log_revenue)) +\n  geom_boxplot() +\n  labs(x = \"Release decade\", y = \"log(Revenue + 1)\",\n       title = \"Distribution of log(Revenue+1) by decade\")\n\n\n\n\n\n\n\ndf_c |&gt;\n  count(release_decade, sort = TRUE)\n\n# A tibble: 4 × 2\n  release_decade     n\n  &lt;chr&gt;          &lt;int&gt;\n1 2000s           2041\n2 2010s           1429\n3 1990s            776\n4 1980s            277\n\nfit_c &lt;- aov(log_revenue ~ release_decade, data = df_c)\nsummary(fit_c)\n\n                 Df Sum Sq Mean Sq F value Pr(&gt;F)  \nrelease_decade    3    520   173.4   2.585 0.0515 .\nResiduals      4519 303217    67.1                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTukeyHSD(fit_c, conf.level = 0.90)\n\n  Tukey multiple comparisons of means\n    90% family-wise confidence level\n\nFit: aov(formula = log_revenue ~ release_decade, data = df_c)\n\n$release_decade\n                  diff        lwr         upr     p adj\n1990s-1980s -0.7129820 -2.0270391  0.60107511 0.5990527\n2000s-1980s -1.2745683 -2.4767390 -0.07239766 0.0716672\n2010s-1980s -0.8541542 -2.0867035  0.37839513 0.3853463\n2000s-1990s -0.5615863 -1.3533797  0.23020703 0.3641933\n2010s-1990s -0.1411722 -0.9783701  0.69602567 0.9803849\n2010s-2000s  0.4204141 -0.2271726  1.06800082 0.4448412\n\ntbl_c &lt;- df_c |&gt;\n  group_by(release_decade) |&gt;\n  summarize(\n    n    = n(),\n    mean = mean(log_revenue),\n    sd   = sd(log_revenue),\n    .groups = \"drop\"\n  )\ntbl_c\n\n# A tibble: 4 × 4\n  release_decade     n  mean    sd\n  &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1980s            277  13.2  7.57\n2 1990s            776  12.5  8.03\n3 2000s           2041  11.9  8.30\n4 2010s           1429  12.3  8.24\n\n\nThis block examines decade differences in revenue on a log scale. The boxplot of log(Revenue + 1) by release_decade suggests modest shifts across decades, and the one-way ANOVA formalizes that pattern by testing equality of decade means. Tukey’s 90% confidence intervals then identify which specific decade pairs differ; interpret any pair whose interval excludes zero as a credible difference at the assignment’s 90% level, and read the sign of the contrast to determine which decade has the higher mean."
  },
  {
    "objectID": "dash1.html#task-d",
    "href": "dash1.html#task-d",
    "title": "dash1",
    "section": "",
    "text": "df_d &lt;- df_filter |&gt;\n  filter(!is.na(english_group)) |&gt;\n  mutate(profit = as.integer(revenue &gt;2.4* budget)) |&gt;\n  select(english_group, profit)\n\ntab_wide &lt;- df_d |&gt;\n  mutate(english_group = factor(english_group, levels = c(\"English\",\"Non-English\"))) |&gt;\n  count(english_group, profit) |&gt;\n  pivot_wider(names_from = profit, values_from = n, values_fill = 0) |&gt;\n  rename(not_profit = `0`, profit = `1`) |&gt;\n  arrange(english_group)\n\ntab_wide \n\n# A tibble: 2 × 3\n  english_group not_profit profit\n  &lt;fct&gt;              &lt;int&gt;  &lt;int&gt;\n1 English             2847   1630\n2 Non-English          222     76\n\na &lt;- tab_wide |&gt; filter(english_group == \"English\")     |&gt; pull(profit)\nb &lt;- tab_wide |&gt; filter(english_group == \"English\")     |&gt; pull(not_profit)\nc &lt;- tab_wide |&gt; filter(english_group == \"Non-English\") |&gt; pull(profit)\nd &lt;- tab_wide |&gt; filter(english_group == \"Non-English\") |&gt; pull(not_profit)\n\n\nc(a=a, b=b, c=c, d=d)\n\n   a    b    c    d \n1630 2847   76  222 \n\np1 &lt;- a/(a+b)  \np2 &lt;- c/(c+d)   \n\nrd_test &lt;- prop.test(x = c(a,c), n = c(a+b, c+d), conf.level = 0.90, correct = TRUE)\nrd_est  &lt;- unname(p1 - p2)\nrd_ci   &lt;- unname(rd_test$conf.int)\n\nRR &lt;- p1/p2\nse_logRR &lt;- sqrt(1/a - 1/(a+b) + 1/c - 1/(c+d))\nz &lt;- qnorm(0.95)  \nRR_ci &lt;- exp(log(RR) + c(-1,1)*z*se_logRR)\n\nor_test &lt;- fisher.test(matrix(c(a,b,c,d), nrow = 2), conf.level = 0.90)\nOR &lt;- unname(or_test$estimate)\nOR_ci &lt;- unname(or_test$conf.int)\n\ntibble::tibble(\n  metric   = c(\"Risk_English (p1)\", \"Risk_NonEnglish (p2)\", \"RD = p1 - p2\", \"RR = p1/p2\", \"OR\"),\n  estimate = c(p1, p2, rd_est, RR, OR),\n  ci90_lwr = c(NA, NA, rd_ci[1], RR_ci[1], OR_ci[1]),\n  ci90_upr = c(NA, NA, rd_ci[2], RR_ci[2], OR_ci[2])\n)\n\n# A tibble: 5 × 4\n  metric               estimate ci90_lwr ci90_upr\n  &lt;chr&gt;                   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Risk_English (p1)       0.364  NA        NA    \n2 Risk_NonEnglish (p2)    0.255  NA        NA    \n3 RD = p1 - p2            0.109   0.0641    0.154\n4 RR = p1/p2              1.43    1.21      1.69 \n5 OR                      1.67    1.33      2.12 \n\ndf_d |&gt;\n  mutate(profit = factor(profit, levels = c(0,1), labels = c(\"Not profit\",\"Profit\"))) |&gt;\n  count(english_group, profit) |&gt;\n  group_by(english_group) |&gt;\n  mutate(pct = n / sum(n)) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = english_group, y = pct, fill = profit)) +\n  geom_col(position = \"fill\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Group\", y = \"Share\", fill = \"\", \n       title = \"Profit vs Not profit proportion by group\")\n\n\n\n\n\n\n\n\nThis block evaluates whether profitability differs by language using a 2×2 table with profit = I(revenue &gt; budget) as the outcome and english_group as the exposure. We report three effect measures with 90% confidence intervals—risk difference (RD), relative risk (RR), and odds ratio (OR). A 90% CI for RD that excludes zero (and RR/OR intervals that exceed one) indicates English-language films have a credibly higher probability of being profitable; if the intervals include the null values, the data do not provide sufficient evidence of a difference at the 90% level."
  },
  {
    "objectID": "dash1.html#task-e",
    "href": "dash1.html#task-e",
    "title": "dash1",
    "section": "",
    "text": "top_k &lt;- 5\n\nsafe_parse_genres &lt;- function(s) {\n  if (is.na(s) || s == \"\" || s == \"[]\") return(list())\n  out &lt;- tryCatch(jsonlite::fromJSON(s), error = function(e) NULL)\n  if (is.null(out)) return(list())\n  if (is.data.frame(out) && \"name\" %in% names(out)) {\n    as.character(out$name)\n  } else if (is.list(out)) {\n    unlist(lapply(out, function(x) tryCatch(as.character(x$name), error = function(e) NA_character_)), use.names = FALSE)\n  } else character()\n}\n\ndf_gen_long &lt;- df_filter |&gt;\n  filter(!is.na(release_decade)) |&gt;\n  mutate(genres_vec = purrr::map(genres, safe_parse_genres)) |&gt;\n  tidyr::unnest_longer(genres_vec, values_to = \"genre\") |&gt;\n  filter(!is.na(genre), genre != \"\")\n\ntop_genres &lt;- df_gen_long |&gt;\n  count(genre, sort = TRUE) |&gt;\n  slice_head(n = top_k) |&gt;\n  pull(genre)\n\nheat_rev_mean &lt;- df_gen_long |&gt;\n  filter(genre %in% top_genres) |&gt;\n  mutate(\n    release_decade = factor(release_decade, levels = c(\"1980s\",\"1990s\",\"2000s\",\"2010s\",\"2020s\")),\n    log_revenue = log1p(revenue)\n  ) |&gt;\n  group_by(release_decade, genre) |&gt;\n  summarize(mean_log_rev = mean(log_revenue), .groups = \"drop\")\n\nheat_rev_mean |&gt;\n  ggplot(aes(x = release_decade, y = genre, fill = mean_log_rev)) +\n  geom_tile() +\n  labs(x = \"Decade\", y = \"Genre (TopK)\",\n       fill = \"Mean log(Rev+1)\",\n       title = \"Mean log(Revenue+1) by decade × genre\")\n\n\n\n\n\n\n\nxtab_counts &lt;- df_gen_long |&gt;\n  filter(genre %in% top_genres) |&gt;\n  count(release_decade, genre) |&gt;\n  tidyr::pivot_wider(names_from = genre, values_from = n, values_fill = 0) |&gt;\n  arrange(factor(release_decade, levels = c(\"1980s\",\"1990s\",\"2000s\",\"2010s\",\"2020s\")))\n\nxtab_counts  \n\n# A tibble: 4 × 6\n  release_decade Action Comedy Drama Romance Thriller\n  &lt;chr&gt;           &lt;int&gt;  &lt;int&gt; &lt;int&gt;   &lt;int&gt;    &lt;int&gt;\n1 1980s              84     82   100      37       71\n2 1990s             200    317   396     168      225\n3 2000s             467    809  1015     436      536\n4 2010s             345    461   646     195      396\n\nxtab_rowprop &lt;- xtab_counts |&gt;\n  tibble::column_to_rownames(\"release_decade\") |&gt;\n  as.matrix() |&gt;\n  prop.table(margin = 1) |&gt;\n  as.data.frame() |&gt;\n  tibble::rownames_to_column(\"release_decade\")\n\nxtab_rowprop \n\n  release_decade    Action    Comedy     Drama    Romance  Thriller\n1          1980s 0.2245989 0.2192513 0.2673797 0.09893048 0.1898396\n2          1990s 0.1531394 0.2427259 0.3032159 0.12863706 0.1722818\n3          2000s 0.1431198 0.2479314 0.3110634 0.13361937 0.1642660\n4          2010s 0.1688693 0.2256486 0.3162017 0.09544787 0.1938326\n\nmat_counts &lt;- xtab_counts |&gt;\n  tibble::column_to_rownames(\"release_decade\") |&gt;\n  as.matrix()\n\nchisq_res &lt;- chisq.test(mat_counts) \n\nThis chunk expands multi-label genres into a long format so each film contributes to all of its tags, selects the Top-K genres by overall frequency, and then maps color to the mean of log(Revenue + 1) within each decade–genre cell. Darker tiles indicate decade–genre combinations associated with higher average revenues on the log scale. The follow-up cross-tabulation reports counts by decade and genre together with row-wise proportions, which show how the composition of popular genres shifts over time (for example, Drama typically occupies the largest share and Comedy the second largest, while smaller genres fluctuate in relative weight). A Pearson chi-square test applied to the count table assesses whether decade and genre are independent; the test rejects independence at conventional levels, implying that the distribution of genres changes across decades. Because the heatmap summarizes average log revenue rather than counts, and because each film contributes to all of its genres, these EDA results should be interpreted descriptively: they highlight where revenue tends to be stronger without implying mutually exclusive group membership or causal effects."
  }
]